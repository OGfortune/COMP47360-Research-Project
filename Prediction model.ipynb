{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8619f2",
   "metadata": {},
   "source": [
    "### Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da4564c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import dayofmonth, month, hour, minute, dayofweek\n",
    "from pyspark.sql.functions import isnan, isnull, sum\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab1989",
   "metadata": {},
   "source": [
    "### Creating Spark APp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c472314",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae3bce",
   "metadata": {},
   "source": [
    "### Reading CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab775c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = spark.read.csv(\"taxi19_cleaned.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "803a962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'congestion_surcharge',\n",
       " 'dropoff_day',\n",
       " 'day_of_month',\n",
       " 'dropoff_month',\n",
       " 'dropoff_hour']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a56bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged = merged.filter(~(col(\"passenger_count\") >5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82776bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged= merged.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "091d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = pd.read_csv(\"taxi_zones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4781185",
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = taxi_zones[taxi_zones[\"borough\"] == \"Manhattan\"]\n",
    "location_id = manhattan[\"LocationID\"].tolist()\n",
    "\n",
    "zone = manhattan[\"zone\"].tolist()\n",
    "\n",
    "manhattan_zones = dict(zip(location_id, zone))\n",
    "manhattan_ids = list(manhattan_zones.keys())\n",
    "#Filtering pickup and dropoff location if either is in manhattan\n",
    "\n",
    "merged =  merged.filter(col(\"DOLocationID\").isin(manhattan_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6bf2a",
   "metadata": {},
   "source": [
    "### Converting datatypes to correct ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "933add90",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount','tolls_amount', \n",
    "                 'improvement_surcharge', 'total_amount', 'busyness']\n",
    "\n",
    "date_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "int_columns = ['passenger_count',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'dropoff_hour',\n",
    " 'dropoff_day',\n",
    " 'dropoff_month',\n",
    " 'day_of_month']\n",
    "\n",
    "category_columns = ['VendorID', 'RatecodeID', 'Store_and_fwd_flag', 'payment_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4f2a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "for column in merged.columns:\n",
    "    if column in float_columns: \n",
    "        merged = merged.withColumn(column, col(column).cast('float'))\n",
    "    elif column in date_columns:\n",
    "        merged = merged.withColumn(column, to_timestamp(regexp_replace(col(column).cast(StringType()), 'T', ' '), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    elif column in int_columns:\n",
    "        merged = merged.withColumn(column, col(column).cast('int'))\n",
    "    else:\n",
    "        merged = merged.withColumn(column, col(column).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d87defa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42d6755c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'congestion_surcharge',\n",
       " 'dropoff_day',\n",
       " 'day_of_month',\n",
       " 'dropoff_month',\n",
       " 'dropoff_hour']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f0628",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import count\n",
    "grouped_df = merged.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week',\n",
    "                            'month'                            \n",
    "                           ).agg(sum('passenger_count').alias('busyness'))\n",
    "\n",
    "grouped_df = merged2.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month').sum('passenger_count').alias('busyness')\n",
    "\n",
    "\n",
    "# Join the grouped DataFrame back to the original DataFrame\n",
    "merged = merged.join(grouped_df, on=['DOLocationID', 'dropoff_hour_of_day','day_of_week',\n",
    "                            'month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e8f4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = merged.groupBy('DOLocationID','dropoff_hour', 'dropoff_day', 'dropoff_month') \\\n",
    "                    .sum('passenger_count') \\\n",
    "                    .withColumnRenamed('sum(passenger_count)', 'busyness')\n",
    "# Join the grouped DataFrame back to the original Da\n",
    "busyness = merged.join(grouped_df, on=['DOLocationID','dropoff_hour', 'dropoff_day', 'dropoff_month'\n",
    "                            ], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39fb2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged= merged.orderBy('tpep_dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3944c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "week = merged.filter(col('tpep_dropoff_datetime') <= '2019-01-02 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70307e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "524347"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1302ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m :  67265543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:======================================================> (65 + 2) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m :  67265543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('m : ', merged.count())\n",
    "print('m : ', busyness.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a314b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d367822c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+----------+------------+--------------------+-----------+------------+-------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|tip_amount|tolls_amount|congestion_surcharge|dropoff_day|day_of_month|dropoff_month|dropoff_hour|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+----------+------------+--------------------+-----------+------------+-------------+------------+\n",
      "|       1| 2019-01-20 17:59:44|  2019-01-20 18:07:51|              1|          1.5|         239|         143|           1|        7.5|       1.0|         0.0|                   0|          1|          20|            1|          18|\n",
      "|       1| 2019-01-21 00:04:57|  2019-01-21 00:14:46|              1|          1.9|         238|         140|           1|        9.0|       2.0|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:12:34|  2019-01-21 00:19:08|              1|         1.15|         114|         231|           1|        6.5|      1.56|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:06:06|  2019-01-21 00:20:07|              5|         4.35|         140|         202|           1|       15.0|      3.26|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:16:45|  2019-01-21 00:24:30|              5|         2.76|         229|         249|           1|       10.0|       1.5|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:10:17|  2019-01-21 00:24:44|              1|         3.59|         230|         144|           2|       13.5|       0.0|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:18:30|  2019-01-21 00:25:28|              1|         1.42|         186|         233|           1|        7.0|      2.49|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       1| 2019-01-21 00:23:33|  2019-01-21 00:27:02|              1|          0.5|          42|          42|           1|        4.5|       1.0|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:23:15|  2019-01-21 00:28:33|              2|         0.87|         113|         114|           1|        5.5|      1.36|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:29:42|  2019-01-21 00:34:01|              1|         1.07|         100|         161|           2|        5.5|       0.0|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:24:12|  2019-01-21 00:36:35|              1|         1.99|         249|          68|           2|       10.0|       0.0|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       1| 2019-01-21 00:24:37|  2019-01-21 00:38:34|              3|          2.6|         114|         162|           1|       12.0|      2.65|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:32:03|  2019-01-21 00:42:37|              2|         2.84|         141|         164|           1|       11.0|      2.46|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:38:02|  2019-01-21 00:54:39|              1|         5.28|         107|         238|           1|       17.0|      3.66|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       1| 2019-01-21 00:55:34|  2019-01-21 00:57:41|              1|          0.4|         113|          79|           1|        3.5|       1.4|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       1| 2019-01-21 00:38:57|  2019-01-21 00:59:37|              1|          4.8|         249|         140|           1|       18.0|      3.85|         0.0|                   0|          2|          21|            1|           0|\n",
      "|       2| 2019-01-21 00:56:19|  2019-01-21 01:04:35|              1|         1.09|         230|         229|           1|        7.0|       2.0|         0.0|                   0|          2|          21|            1|           1|\n",
      "|       1| 2019-01-21 01:05:22|  2019-01-21 01:05:56|              1|          0.2|         229|         162|           2|        2.5|       0.0|         0.0|                   0|          2|          21|            1|           1|\n",
      "|       1| 2019-01-21 00:56:11|  2019-01-21 01:06:45|              1|          2.0|         107|         125|           1|        9.5|      2.15|         0.0|                   0|          2|          21|            1|           1|\n",
      "|       2| 2019-01-21 00:55:11|  2019-01-21 01:08:15|              2|         3.68|         164|         239|           2|       13.5|       0.0|         0.0|                   0|          2|          21|            1|           1|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+----------+------------+--------------------+-----------+------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb65758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109886427"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_passenger_count = grouped_df.select(sum('busyness')).collect()[0][0]\n",
    "total_passenger_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1deeddb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_df2 = taxi18.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month') \\\n",
    "                    .sum('passenger_count') \\\n",
    "                    .withColumnRenamed('sum(passenger_count)', 'busyness')\n",
    "# Join the grouped DataFrame back to the original DataFrame\n",
    "taxi18 = taxi18.join(grouped_df, on=['DOLocationID', 'dropoff_hour_of_day','day_of_week','day_of_month',\n",
    "                            'month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d088cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select distinct values from 'busyness' column and convert to a list\n",
    "distinct_busyness_list = df_with_busyness.select(col('busyness')).distinct().collect()\n",
    "distinct_busyness_list = [row['busyness'] for row in distinct_busyness_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10f8ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_grouped = merged.groupBy('DOLocationID', 'dropoff_hour_of_day', 'month', 'day_of_week') \\\n",
    "               .agg(sum('passenger_count').alias('busyness')) \\\n",
    "               .select('DOLocationID', 'dropoff_hour_of_day', 'month', 'day_of_week', 'busyness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03ed69ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7382"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distinct_busyness_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b834696",
   "metadata": {},
   "source": [
    "### General ALL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c91b915a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/09 23:35:00 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/09 23:35:00 WARN BlockManager: Persisting block rdd_206_0 to disk instead.\n",
      "23/07/09 23:38:16 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/09 23:40:03 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/09 23:41:42 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/09 23:43:31 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/09 23:45:17 WARN MemoryStore: Not enough space to cache rdd_206_0 in memory! (computed 4.1 GiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|busyness|        prediction|\n",
      "+--------+------------------+\n",
      "|  5240.0| 3177.641486960656|\n",
      "|  1954.0|2504.2754554649746|\n",
      "|  1939.0|2504.2754554649746|\n",
      "|  2396.0|1906.0266136001214|\n",
      "|  2162.0|2720.2507188221207|\n",
      "|   856.0|2504.2754554649746|\n",
      "|   275.0|2504.2754554649746|\n",
      "|   897.0|2504.2754554649746|\n",
      "|   476.0|   372.45373024402|\n",
      "|  1251.0|2322.4567904752025|\n",
      "|   323.0| 2995.822821970884|\n",
      "|  5831.0|2720.2507188221207|\n",
      "|  1871.0|2504.2754554649746|\n",
      "|  1757.0| 3177.641486960656|\n",
      "|  2386.0|2720.2507188221207|\n",
      "|  3875.0|2720.2507188221207|\n",
      "|    29.0|2169.2755786044845|\n",
      "|  6335.0|2720.2507188221207|\n",
      "|  3689.0| 3663.849604650041|\n",
      "|  1917.0|2720.2507188221207|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 1630.140224082292\n",
      "Mean Absolute Error (MAE): 1270.2846925770416\n",
      "R-squared (R2): 0.3290205213096953\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['PULocationID', 'DOLocationID', 'passenger_count','fare_amount',\n",
    "                     'tip_amount', 'tolls_amount','trip_distance','day_of_month',\n",
    "                     'dropoff_hour_of_day', 'day_of_week', 'month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(merged).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49821002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: dropoff_hour_of_day, Importance: 0.31561950397606175\n",
      "Feature: DOLocationID, Importance: 0.2859065808401694\n",
      "Feature: trip_distance, Importance: 0.23859211646214631\n",
      "Feature: fare_amount, Importance: 0.055598592952849246\n",
      "Feature: PULocationID, Importance: 0.04561896241373737\n",
      "Feature: month, Importance: 0.030446803821005862\n",
      "Feature: day_of_week, Importance: 0.02584469632081911\n",
      "Feature: tip_amount, Importance: 0.0023727432132108865\n",
      "Feature: passenger_count, Importance: 0.0\n",
      "Feature: tolls_amount, Importance: 0.0\n",
      "Feature: day_of_month, Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already trained the model and made predictions\n",
    "\n",
    "# Get the trained RandomForestRegressor model from the pipeline\n",
    "trained_model = model.stages[0]\n",
    "\n",
    "# Get the feature importance values\n",
    "feature_importance = trained_model.featureImportances\n",
    "\n",
    "# Create a list of tuples with feature names and their importance values\n",
    "feature_importance_list = [(feature, importance) for feature, importance in zip(selected_features, feature_importance)]\n",
    "\n",
    "# Sort the feature importance list by importance values in descending order\n",
    "sorted_feature_importance = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance list\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e606f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f40fa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2 = spark.read.csv(\"new_sort.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e743d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\"\"\"grouped_df = merged2.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week',\n",
    "                            'month'                            \n",
    "                           ).agg(sum('passenger_count').alias('busyness'))\"\"\"\n",
    "grouped_df = merged.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month').sum('passenger_count').alias('busyness')\n",
    "\n",
    "\n",
    "# Join the grouped DataFrame back to the original DataFrame\n",
    "merged2 = merged2.join(grouped_df, on=['DOLocationID', 'dropoff_hour_of_day','day_of_week',\n",
    "                            'month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97a2e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select distinct values from 'busyness' column and convert to a list\n",
    "distinct_busyness_list1 = merged2.select(col('busyness')).distinct().collect()\n",
    "distinct_busyness_list1 = [row['busyness'] for row in distinct_busyness_list1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf39accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "for column in merged2.columns:\n",
    "    if column in float_columns: \n",
    "        merged2 = merged2.withColumn(column, col(column).cast('float'))\n",
    "    elif column in date_columns:\n",
    "        merged2 = merged2.withColumn(column, to_timestamp(regexp_replace(col(column).cast(StringType()), 'T', ' '), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    elif column in int_columns:\n",
    "        merged2 = merged2.withColumn(column, col(column).cast('int'))\n",
    "    else:\n",
    "        merged2 = merged2.withColumn(column, col(column).cast(StringType()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dc823e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'fare_amount',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'journey_length_in_minutes',\n",
       " 'day_of_month',\n",
       " 'dropoff_minute_of_hour',\n",
       " 'pickup_minute_of_hour',\n",
       " 'dropoff_hour_of_day',\n",
       " 'pickup_hour_of_day',\n",
       " 'day_of_week',\n",
       " 'month']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e389aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/11 10:32:57 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/11 10:32:57 WARN BlockManager: Persisting block rdd_354_0 to disk instead.\n",
      "23/07/11 10:34:03 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/11 10:34:40 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/11 10:35:19 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/11 10:35:59 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "23/07/11 10:36:40 WARN MemoryStore: Not enough space to cache rdd_354_0 in memory! (computed 4.1 GiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|busyness|        prediction|\n",
      "+--------+------------------+\n",
      "|       8| 253.1792791993746|\n",
      "|     158| 258.9214328374647|\n",
      "|     126| 334.2635796503811|\n",
      "|     790|  488.776433393954|\n",
      "|     515| 515.5278005958105|\n",
      "|     558|  365.539249323331|\n",
      "|     233|334.89102936410745|\n",
      "|     187| 345.7211654202948|\n",
      "|      46|255.96326191881093|\n",
      "|     846| 410.1282851547487|\n",
      "|     248| 382.4267910374152|\n",
      "|     284|222.70836173148197|\n",
      "|     559|  535.709833230047|\n",
      "|      68| 370.5612437741019|\n",
      "|     117| 363.2063754310085|\n",
      "|     472| 373.1270266110638|\n",
      "|     480| 363.2063754310085|\n",
      "|     331| 399.5306963216479|\n",
      "|     380| 532.1099580339934|\n",
      "|       4|231.62493852530557|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 245.3499950858821\n",
      "Mean Absolute Error (MAE): 187.72172456109138\n",
      "R-squared (R2): 0.32581389659718585\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['PULocationID', 'DOLocationID', 'passenger_count','fare_amount',\n",
    "                     'tip_amount', 'tolls_amount','trip_distance','dropoff_hour', 'dropoff_day', 'dropoff_month', 'day_of_month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(merged).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ffa111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7877"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distinct_busyness_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94489214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7877"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.select(col('busyness')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a86dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|busyness|        prediction|\n",
      "+--------+------------------+\n",
      "|      59| 47.45243096967346|\n",
      "|      11|138.03111921435945|\n",
      "|      17|143.47356133290634|\n",
      "|     162|138.03111921435945|\n",
      "|     280|154.40721865713093|\n",
      "|      41|166.97667842978294|\n",
      "|     267|147.06870174599013|\n",
      "|     163|166.07047140841624|\n",
      "|       2| 35.32143266701962|\n",
      "|     420|209.57475357423579|\n",
      "|     352| 223.4541572222643|\n",
      "|      47| 47.45243096967346|\n",
      "|     227|  79.2823421931985|\n",
      "|      64| 75.29488309067432|\n",
      "|      10| 59.14717331350285|\n",
      "|      14|  79.4248013912474|\n",
      "|      69| 129.8899557312832|\n",
      "|      35| 47.45243096967346|\n",
      "|      45|  79.2823421931985|\n",
      "|      58| 129.8899557312832|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 208:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 177.8238185948071\n",
      "Mean Absolute Error (MAE): 129.9281161012205\n",
      "R-squared (R2): 0.3879057736174324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['DOLocationID','dropoff_hour', 'dropoff_day', 'dropoff_month', 'day_of_month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(grouped_df).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68df374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:======================================================>(64 + 1) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|dropoff_day|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          6|\n",
      "|          3|\n",
      "|          5|\n",
      "|          4|\n",
      "|          7|\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_df.select(col('dropoff_day')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b409c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df= grouped_df.orderBy('dropoff_hour',\n",
    " 'dropoff_day',\n",
    " 'dropoff_month',\n",
    " 'day_of_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e78c708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOLocationID',\n",
       " 'dropoff_hour',\n",
       " 'dropoff_day',\n",
       " 'dropoff_month',\n",
       " 'day_of_month',\n",
       " 'busyness']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4ad72",
   "metadata": {},
   "source": [
    "### Grouped DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ecb888",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "dropoff_hour_of_day does not exist. Available: DOLocationID, dropoff_hour, dropoff_day, dropoff_month, day_of_month, busyness",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/l9wg2z0s1_1050p97phv554r0000gn/T/ipykernel_1553/1924981895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Assemble the features into a vector column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0massembled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Split the data into training and testing sets (70% for training, 30% for testing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: dropoff_hour_of_day does not exist. Available: DOLocationID, dropoff_hour, dropoff_day, dropoff_month, day_of_month, busyness"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(grouped_df).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49cc86f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "dropoff_hour_of_day does not exist. Available: DOLocationID, dropoff_hour, dropoff_day, dropoff_month, day_of_month, busyness",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/l9wg2z0s1_1050p97phv554r0000gn/T/ipykernel_1553/1924981895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Assemble the features into a vector column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0massembled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Split the data into training and testing sets (70% for training, 30% for testing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: dropoff_hour_of_day does not exist. Available: DOLocationID, dropoff_hour, dropoff_day, dropoff_month, day_of_month, busyness"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(grouped_df).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fbe8ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+-----+--------+\n",
      "|DOLocationID|dropoff_hour_of_day|day_of_week|month|busyness|\n",
      "+------------+-------------------+-----------+-----+--------+\n",
      "|          68|                 10|          1|    1|    3129|\n",
      "|          90|                 16|          1|    1|    3803|\n",
      "|         249|                 19|          1|    1|    4526|\n",
      "|          68|                 23|          2|    1|    3128|\n",
      "|         164|                  9|          3|    1|    6235|\n",
      "|         144|                 11|          3|    1|    1873|\n",
      "|         231|                 16|          3|    1|    3062|\n",
      "|         233|                 17|          3|    1|    2977|\n",
      "|          48|                  7|          4|    1|    2551|\n",
      "|         239|                 11|          4|    1|    4114|\n",
      "|         243|                 14|          4|    1|     173|\n",
      "|         113|                 22|          4|    1|    3447|\n",
      "|         141|                 10|          5|    1|    2985|\n",
      "|         249|                 15|          5|    1|    2055|\n",
      "|          68|                  2|          6|    1|    1299|\n",
      "|          75|                 13|          6|    1|    2007|\n",
      "|         232|                  0|          7|    1|    1382|\n",
      "|         230|                 12|          7|    1|    6911|\n",
      "|         186|                 18|          7|    1|    6791|\n",
      "|         143|                 15|          1|    1|    3185|\n",
      "+------------+-------------------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b531c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|busyness|        prediction|\n",
      "+--------+------------------+\n",
      "|     182|192.60974411828676|\n",
      "|      11|183.02707651089622|\n",
      "|      22| 36.93785143716613|\n",
      "|     229|192.60974411828676|\n",
      "|     187|156.88475311678036|\n",
      "|      10|  92.1977939321148|\n",
      "|       8| 90.32598484348091|\n",
      "|     102|183.02707651089622|\n",
      "|      11| 36.93785143716613|\n",
      "|     366|204.04156114740928|\n",
      "|      15|155.89170478584305|\n",
      "|     216| 207.7164088501429|\n",
      "|     448|271.34246957403315|\n",
      "|     466|158.22854126732014|\n",
      "|     547|  248.982188505142|\n",
      "|     136|223.56363170926045|\n",
      "|     518|246.72098736462448|\n",
      "|     480|271.34246957403315|\n",
      "|     251| 225.2649509667736|\n",
      "|     305|223.56363170926045|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 422:==============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 203.2281787477033\n",
      "Mean Absolute Error (MAE): 146.4695838133056\n",
      "R-squared (R2): 0.307839082920046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'day_of_month','month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(grouped_df2).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71845c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df= grouped_df2.orderBy('dropoff_hour_of_day', 'day_of_week', 'day_of_month', 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb150f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|busyness|        prediction|\n",
      "+--------+------------------+\n",
      "|     120|137.52713478997873|\n",
      "|      82|   129.61040607411|\n",
      "|     154|   129.61040607411|\n",
      "|     182|139.70813745229728|\n",
      "|      11|139.70813745229728|\n",
      "|      14|139.70813745229728|\n",
      "|     229|139.70813745229728|\n",
      "|     187|142.34056395789304|\n",
      "|     165|139.70813745229728|\n",
      "|     102|139.70813745229728|\n",
      "|      92|142.34056395789304|\n",
      "|     120|139.70813745229728|\n",
      "|     157|142.34056395789304|\n",
      "|      56| 194.5580890455339|\n",
      "|     112|198.98400742775397|\n",
      "|     257|198.98400742775397|\n",
      "|     366|198.98400742775397|\n",
      "|      32|198.98400742775397|\n",
      "|      48|198.98400742775397|\n",
      "|    1014| 194.5580890455339|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 620:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 243.38463414167302\n",
      "Mean Absolute Error (MAE): 180.3343209058462\n",
      "R-squared (R2): 0.11190836619367084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a Spark DataFrame named \"merged\" with features and target column\n",
    "# Select the relevant columns\n",
    "selected_features = ['DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'day_of_month','month']\n",
    "target_column = 'busyness'\n",
    "\n",
    "# Assemble the features into a vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(ordered_df).select(\"features\", target_column)\n",
    "\n",
    "# Split the data into training and testing sets (70% for training, 30% for testing)\n",
    "train_data = assembled_df.limit(int(assembled_df.count() * 0.7))\n",
    "test_data = assembled_df.subtract(train_data)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(numTrees=2, featuresCol=\"features\", labelCol=target_column)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[regressor])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select the predicted and actual values\n",
    "result = predictions.select(target_column, \"prediction\")\n",
    "\n",
    "# Show the predicted and actual values\n",
    "result.show()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36991f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.filter(col('passenger_count')< 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ab72e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "555330"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21799bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "524714"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73077caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 701:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+-------------------------+------------+----------------------+---------------------+-------------------+------------------+-----------+-----+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|tip_amount|tolls_amount|journey_length_in_minutes|day_of_month|dropoff_minute_of_hour|pickup_minute_of_hour|dropoff_hour_of_day|pickup_hour_of_day|day_of_week|month|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+-------------------------+------------+----------------------+---------------------+-------------------+------------------+-----------+-----+\n",
      "| 2017-01-01 00:00:55|  2017-01-01 00:14:52|              2|         4.41|         233|          74|       14.5|       2.0|         0.0|                    13.95|           1|                    14|                    0|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:02:22|  2017-01-01 00:19:28|              2|          6.1|         114|         262|       20.0|      10.0|         0.0|                     17.1|           1|                    19|                    2|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:06:29|  2017-01-01 00:17:01|              2|         1.45|         249|          68|        8.5|       0.0|         0.0|                    10.53|           1|                    17|                    6|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:07:54|  2017-01-01 00:17:34|              1|         1.86|         141|         233|        8.5|      2.45|         0.0|                     9.67|           1|                    17|                    7|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:10:05|  2017-01-01 00:18:54|              2|          3.2|         263|         170|       11.5|       0.0|         0.0|                     8.82|           1|                    18|                   10|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:10:18|  2017-01-01 00:32:16|              1|         5.98|         142|         148|       21.0|      4.46|         0.0|                    21.97|           1|                    32|                   10|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:13:06|  2017-01-01 00:25:33|              1|         3.16|         170|         263|       12.0|      2.66|         0.0|                    12.45|           1|                    25|                   13|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:13:38|  2017-01-01 00:22:19|              2|         2.32|         161|         263|        9.5|       0.0|         0.0|                     8.68|           1|                    22|                   13|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:13:45|  2017-01-01 00:20:13|              5|         1.44|         239|         143|        7.0|      1.24|         0.0|                     6.47|           1|                    20|                   13|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:14:43|  2017-01-01 00:32:48|              1|         4.61|         114|         229|       17.5|      3.76|         0.0|                    18.08|           1|                    32|                   14|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:19:04|  2017-01-01 00:39:19|              1|         4.16|          48|         231|       17.0|       0.0|         0.0|                    20.25|           1|                    39|                   19|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:20:10|  2017-01-01 00:42:41|              1|         1.84|         161|         107|       14.5|      3.95|         0.0|                    22.52|           1|                    42|                   20|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:20:24|  2017-01-01 00:39:22|              1|         1.04|         142|          48|       12.0|      3.32|         0.0|                    18.97|           1|                    39|                   20|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:20:50|  2017-01-01 00:24:55|              1|         1.19|         142|         238|        5.5|       2.0|         0.0|                     4.08|           1|                    24|                   20|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:20:52|  2017-01-01 00:59:43|              2|          4.7|         236|         211|       25.0|      5.25|         0.0|                    38.85|           1|                    59|                   20|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:21:27|  2017-01-01 00:34:09|              1|         1.74|         142|         229|       10.0|      2.26|         0.0|                     12.7|           1|                    34|                   21|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:22:28|  2017-01-01 00:39:45|              3|         1.42|         249|          68|       11.5|      2.56|         0.0|                    17.28|           1|                    39|                   22|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:22:31|  2017-01-01 00:38:57|              2|         0.92|          79|         234|       10.5|       0.0|         0.0|                    16.43|           1|                    38|                   22|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:23:33|  2017-01-01 00:48:12|              2|          2.5|         158|         137|       16.0|       0.0|         0.0|                    24.65|           1|                    48|                   23|                  0|                 0|          1|    1|\n",
      "| 2017-01-01 00:23:37|  2017-01-01 00:39:13|              2|          2.3|         234|         211|       12.0|       0.0|         0.0|                     15.6|           1|                    39|                   23|                  0|                 0|          1|    1|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+-------------------------+------------+----------------------+---------------------+-------------------+------------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51efcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df2 = taxi18.groupBy('DOLocationID', 'dropoff_hour_of_day', 'day_of_week', 'month') \\\n",
    "                    .sum('passenger_count') \\\n",
    "                    .withColumnRenamed('sum(passenger_count)', 'busyness')\n",
    "# Join the grouped DataFrame back to the original DataFrame\n",
    "taxi18 = taxi18.join(grouped_df, on=['DOLocationID', 'dropoff_hour_of_day','day_of_week','day_of_month',\n",
    "                            'month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87ce5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "week = merged.filter(~(col('tpep_dropoff_datetime')> \"2019-01-02 00:00:00\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a677b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ae84de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68971099"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14d29ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d050ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 385:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+-------------+------------+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+--------------------+--------+\n",
      "|DOLocationID|dropoff_hour|dropoff_day|dropoff_month|day_of_month|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|payment_type|fare_amount|tip_amount|tolls_amount|congestion_surcharge|busyness|\n",
      "+------------+------------+-----------+-------------+------------+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+--------------------+--------+\n",
      "|         141|          21|          5|            1|           3|       1| 2019-01-03 21:43:46|  2019-01-03 21:45:47|              1|          0.6|         263|           1|        4.0|       1.5|         0.0|                   0|       1|\n",
      "|         151|          21|          5|            1|           3|       1| 2019-01-03 21:49:22|  2019-01-03 21:59:12|              1|          2.2|         263|           1|       10.0|       1.0|         0.0|                   0|       1|\n",
      "|         246|          22|          5|            1|           3|       1| 2019-01-03 22:04:40|  2019-01-03 22:16:15|              1|          2.5|         238|           1|       11.0|      2.35|         0.0|                   0|       1|\n",
      "|         261|           4|          7|            1|           5|       1| 2019-01-05 03:50:35|  2019-01-05 04:00:41|              1|          1.9|         148|           1|        9.0|      2.05|         0.0|                   0|       1|\n",
      "|         249|           4|          7|            1|           5|       1| 2019-01-05 04:09:40|  2019-01-05 04:14:28|              2|          0.3|         113|           1|        4.5|       1.4|         0.0|                   0|       2|\n",
      "|         143|           6|          6|            1|          18|       1| 2019-01-18 06:46:26|  2019-01-18 06:53:10|              1|          0.8|         163|           2|        6.5|       0.0|         0.0|                   0|       1|\n",
      "|         161|           7|          6|            1|          18|       1| 2019-01-18 07:06:46|  2019-01-18 07:14:40|              1|          1.2|         186|           1|        7.5|      1.65|         0.0|                   0|       2|\n",
      "|         161|           7|          6|            1|          18|       1| 2019-01-18 07:32:39|  2019-01-18 07:44:21|              1|          1.8|          68|           1|        9.0|      1.95|         0.0|                   0|       2|\n",
      "|         237|           7|          6|            1|          18|       1| 2019-01-18 07:50:53|  2019-01-18 07:57:27|              1|          1.1|         170|           1|        6.5|      1.45|         0.0|                   0|       1|\n",
      "|         234|           8|          6|            1|          18|       1| 2019-01-18 08:00:34|  2019-01-18 08:11:05|              1|          1.9|         237|           2|        9.5|       0.0|         0.0|                   0|       1|\n",
      "|         170|           8|          6|            1|          18|       1| 2019-01-18 08:12:22|  2019-01-18 08:15:18|              1|          0.5|         107|           2|        4.0|       0.0|         0.0|                   0|       3|\n",
      "|         170|           8|          6|            1|          18|       1| 2019-01-18 08:17:23|  2019-01-18 08:21:02|              1|          0.5|         137|           1|        4.5|      1.05|         0.0|                   0|       3|\n",
      "|         100|           8|          6|            1|          18|       1| 2019-01-18 08:23:10|  2019-01-18 08:27:51|              1|          0.8|         170|           1|        5.0|      1.15|         0.0|                   0|       1|\n",
      "|         170|           8|          6|            1|          18|       1| 2019-01-18 08:32:52|  2019-01-18 08:44:00|              1|          1.0|         186|           1|        8.5|      1.85|         0.0|                   0|       3|\n",
      "|         249|           9|          6|            1|          18|       1| 2019-01-18 08:46:37|  2019-01-18 09:03:46|              1|          2.3|         170|           1|       12.5|       1.5|         0.0|                   0|       1|\n",
      "|         164|           9|          6|            1|          18|       1| 2019-01-18 09:28:12|  2019-01-18 09:49:21|              1|          2.3|         158|           1|       14.0|      2.95|         0.0|                   0|       1|\n",
      "|         234|           9|          6|            1|          18|       1| 2019-01-18 09:55:26|  2019-01-18 09:57:38|              1|          0.3|         234|           1|        3.5|      0.85|         0.0|                   0|       1|\n",
      "|          13|          10|          6|            1|          18|       1| 2019-01-18 10:23:48|  2019-01-18 10:33:15|              1|          1.5|         158|           1|        8.5|      1.85|         0.0|                   0|       1|\n",
      "|         113|          11|          6|            1|          18|       1| 2019-01-18 10:39:23|  2019-01-18 11:00:08|              1|          2.6|          13|           1|       15.0|      3.15|         0.0|                   0|       1|\n",
      "|         234|          11|          6|            1|          18|       1| 2019-01-18 11:18:01|  2019-01-18 11:26:15|              1|          0.9|         144|           1|        7.0|      1.55|         0.0|                   0|       1|\n",
      "+------------+------------+-----------+-------------+------------+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+----------+------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87cc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
